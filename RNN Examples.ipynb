{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of RNN Approaches\n",
    "(see https://distill.pub/2016/augmented-rnns/)\n",
    "\n",
    "This creates a sample task, based on number sequences, and\n",
    "different goals to compare the performance of vanilla RNN's,\n",
    "LSTM's, NTM's, and attention based networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Numbers:\n",
    "    ''' This is a controller \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, max_number = 9,\n",
    "                 increment_func = lambda stream: stream[-1]+1,\n",
    "                 reset_func = None,\n",
    "                 reset_value_func = lambda stream: 0,\n",
    "                 goal_func = lambda stream: stream[0]):\n",
    "        ''' Initialize the controller\n",
    "        Args:\n",
    "            increment_func: called on each number in the stream to decide\n",
    "                the next number in the stream\n",
    "            reset_func: evalutes the stream to determine if a reset\n",
    "                condition has been met causing a restart on a new number\n",
    "            reset_value_func: if a reset is triggered, this is called to\n",
    "                determine the new value to restart the stream with\n",
    "            goal_func: a function that returns the target output value\n",
    "                for the stream (i.e. the goal output)\n",
    "        '''\n",
    "        self.max_number = max_number\n",
    "        self.all_numbers = list(range(max_number+1))\n",
    "        self.all_classes = self.all_numbers + [-1]\n",
    "        \n",
    "        self.increment_func = increment_func\n",
    "        self.reset_func = reset_func\n",
    "        self.reset_value_func = reset_value_func\n",
    "        self.goal_func = goal_func\n",
    "        \n",
    "        \n",
    "    def create_stream(self, length, seed_stream = [0]):\n",
    "        ''' Create a stream (sequence) of numbers\n",
    "        \n",
    "        Args:\n",
    "            length: length of the stream to generate\n",
    "            seed_stream: seed stream to use in generating the first number\n",
    "                \n",
    "        Returns:\n",
    "            A sequence (list) of numbers in the stream\n",
    "        '''\n",
    "        if self.reset_func == None:\n",
    "            self.reset_func = lambda stream: stream[-1] == self.max_number\n",
    "    \n",
    "        start_num = self.reset_value_func(seed_stream)\n",
    "        stream = [start_num]\n",
    "        \n",
    "        for i in range(max(0,length-1)):\n",
    "            if self.reset_func(stream):\n",
    "                new_num = self.reset_value_func(stream)\n",
    "            else:\n",
    "                new_num = self.increment_func(stream)\n",
    "            stream.append(new_num)\n",
    "            \n",
    "        return stream\n",
    "    \n",
    "    \n",
    "    def encode_stream(self, num_stream):\n",
    "        ''' Converts number or number_stream into one-hot encoded tensor\n",
    "        An additional category is added for OTHER (unrecognized numbers)\n",
    "        \n",
    "        Args:\n",
    "            num_stream: A single number of sequence (list) of numbers\n",
    "                that will be encoded\n",
    "            \n",
    "        Returns:\n",
    "            Encoded tensor of shape (number_index, _, one_hot_encoded_number)\n",
    "        '''\n",
    "        num_stream = num_stream if type(num_stream) is list else [num_stream]\n",
    "        output = torch.zeros(len(num_stream), 1, len(self.all_numbers)+1)\n",
    "        \n",
    "        for i, num in enumerate(num_stream):\n",
    "            value = num if num in self.all_numbers else -1\n",
    "            ind = self.all_classes.index(value)\n",
    "            output[i,0,ind] = 1\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def get_stream_goal(self, stream):\n",
    "        ''' Return the goal output for the providede stream \n",
    "        Args:\n",
    "            stream: a list of numbers in the sequence\n",
    "        \n",
    "        Returns:\n",
    "            Simple evaluation of the goal function (type depends\n",
    "            on the goal function provided)\n",
    "        '''\n",
    "        return self.goal_func(stream)\n",
    "    \n",
    "    \n",
    "    def decode_row(self, encoded_number):\n",
    "        ''' Decode a one-hot encoded vector back to the number '''\n",
    "        ind = encoded_number.argmax()\n",
    "        value = self.all_numbers[ind] if ind in self.all_numbers else -1\n",
    "        return value\n",
    "    \n",
    "        \n",
    "    def get_dim(self):\n",
    "        ''' Return dimension of each encoded tensor '''\n",
    "        return len(self.all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    ''' Basic RNN with a softmax output layer '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        ''' Init the RNN '''\n",
    "        super(BasicRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_to_output = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.input_to_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        ''' Run a forward path for a single number of sequence of numbers\n",
    "        \n",
    "        Returns:\n",
    "            The output and hidden layer at the termination of the sequence\n",
    "        '''\n",
    "        for i in range(input.shape[0]):\n",
    "            combined_input = torch.cat((input[i].unsqueeze(0), hidden), dim=2)\n",
    "            hidden = self.input_to_hidden(combined_input)\n",
    "            \n",
    "        output = self.input_to_output(combined_input)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        ''' Returns new hidden layers for the start of a new sequence '''\n",
    "        model_device = next(self.parameters()).device\n",
    "        return torch.zeros(1, 1, self.hidden_size).to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, number_tool, criterion, optim, train_size = 100e3, stream_size = 200, print_interval = 1e3):\n",
    "    ''' Runs a full training pass for a given model '''\n",
    "    train_size = int(train_size)\n",
    "    print_interval = int(print_interval)\n",
    "    error_sum = 0\n",
    "    model_device = next(model.parameters()).device\n",
    "\n",
    "    for i in range(train_size):\n",
    "        # Generate a new random sequence for training\n",
    "        stream = number_tool.create_stream(stream_size)\n",
    "        obs = number_tool.encode_stream(stream).to(model_device)\n",
    "\n",
    "        # Reset the model gradients and hidden layer\n",
    "        model.zero_grad()\n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Score the model\n",
    "        output, hidden = model.forward(obs, hidden)\n",
    "\n",
    "        # Determine the target output and calculate the loss\n",
    "        goal = number_tool.get_stream_goal(stream)\n",
    "        target = torch.LongTensor([goal]).to(model_device)\n",
    "        loss = criterion(output.squeeze(0), target)\n",
    "        \n",
    "        # Run backprop with the errors and update model/trackers\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        error_sum += loss\n",
    "\n",
    "        if i % print_interval == 0:\n",
    "            print(f'[{i}] Error: {error_sum / print_interval}')\n",
    "            error_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number Generator + Goal Setup\n",
    "max_number = 9\n",
    "goal_func = lambda stream: stream[0]\n",
    "goal_dim = max_number+2\n",
    "\n",
    "number_tool = Numbers(max_number,\n",
    "                      reset_value_func = lambda x: randint(0,max_number),\n",
    "                      goal_func = goal_func)\n",
    "\n",
    "# Model Setup\n",
    "rnn = BasicRNN(number_tool.get_dim(), 5, goal_dim)\n",
    "criterion = nn.NLLLoss()\n",
    "rnn_optim = torch.optim.SGD(rnn.parameters(), lr = 0.001)\n",
    "\n",
    "# Training Size\n",
    "train_size = 100e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_setup_kwargs = {\n",
    "    'train_size': train_size,\n",
    "    'model': rnn,\n",
    "    'number_tool': number_tool,\n",
    "    'criterion': criterion,\n",
    "    'optim': rnn_optim,\n",
    "    'stream_size': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(**rnn_setup_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM(nn.Module):\n",
    "    ''' Basic single-LSTM with output calculated using a single\n",
    "        linear layer with a softmax activation '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        ''' Init the LSTM '''\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.hidden_to_output = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        ''' Run a forward path for a single number of sequence of numbers\n",
    "        \n",
    "        Returns:\n",
    "            The output and hidden layer at the termination of the sequence\n",
    "        '''\n",
    "        # Hidden is made up of (hidden, cell_state)\n",
    "        history, hidden = self.lstm.forward(input, hidden)\n",
    "        output = self.hidden_to_output(hidden[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        ''' Returns new hidden layers for the start of a new sequence '''\n",
    "        model_device = next(self.parameters()).device\n",
    "        return (torch.randn(1, 1, self.hidden_size).to(model_device),\n",
    "                torch.randn(1, 1, self.hidden_size).to(model_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(number_tool.get_dim(), 5, goal_dim)\n",
    "lstm.to(device)\n",
    "\n",
    "lstm_optim = torch.optim.SGD(lstm.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_setup_kwargs = rnn_setup_kwargs.copy()\n",
    "lstm_setup_kwargs['model'] = lstm\n",
    "lstm_setup_kwargs['optim'] = lstm_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(**lstm_setup_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes = ['A','A#','B','C','C#','D','D#','E','F','F#','G','G#','PAUSE']\n",
    "\n",
    "class Music:\n",
    "    def encode_note(note):\n",
    "        ''' One-hot encode the note if present otherwise return all zeros '''\n",
    "        one_hot_vector = torch.zeros(1, len(all_notes))\n",
    "        if note in all_notes:\n",
    "            ind = notes.index(note)\n",
    "            one_hot_vector[0,ind] = 1\n",
    "        return one_hot_vector\n",
    "    \n",
    "    def decode_vector(encoded_note):\n",
    "        ''' Decode a one-hot encoded vector '''\n",
    "        note_index = encoded_note.argmax()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
