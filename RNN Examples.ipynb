{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of RNN Approaches\n",
    "(see https://distill.pub/2016/augmented-rnns/)\n",
    "\n",
    "This creates a sample task, based on number sequences, and\n",
    "different goals to compare the performance of vanilla RNN's,\n",
    "LSTM's, NTM's, and attention based networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, einsum\n",
    "from random import randint\n",
    "from torch.nn import functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import Numbers\n",
    "\n",
    "from training import train_model\n",
    "\n",
    "from models import BasicRNN\n",
    "from models import BasicLSTM\n",
    "from models import NTM_LSTM\n",
    "\n",
    "from models.MemoryNN import MemoryNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number Generator + Goal Setup\n",
    "max_number = 9\n",
    "\n",
    "number_tool = Numbers(\n",
    "    max_number,\n",
    "    increment_func = lambda incr: randint(0,9), # lambda x: randint(0,max_number),\n",
    "    goal_func = lambda stream: stream[0:3] # lambda stream: stream[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Size\n",
    "train_size = 100e3\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_kwargs = {\n",
    "    'train_size': train_size,\n",
    "    'number_tool': number_tool,\n",
    "    'criterion': criterion,\n",
    "    'stream_size': 200,\n",
    "    'model': None,\n",
    "    'optim': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Vanilla RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = BasicRNN(number_tool.get_dim(), 5, number_tool.get_dim(), output_length = 3)\n",
    "rnn.to(device)\n",
    "\n",
    "rnn_optim = torch.optim.SGD(rnn.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "rnn_kwargs = setup_kwargs.copy()\n",
    "rnn_kwargs['model'] = rnn\n",
    "rnn_kwargs['optim'] = rnn_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(**rnn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(number_tool.get_dim(), 5, number_tool.get_dim(), output_length = 3)\n",
    "lstm.to(device)\n",
    "\n",
    "lstm_optim = torch.optim.SGD(lstm.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "lstm_kwargs = setup_kwargs.copy()\n",
    "lstm_kwargs['model'] = lstm\n",
    "lstm_kwargs['optim'] = lstm_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(**lstm_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Turing Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm = NTM_LSTM(number_tool.get_dim(), 5, number_tool.get_dim(),\n",
    "               10, 10, output_length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_banks = 10\n",
    "memory_dim = 20\n",
    "\n",
    "ntm = NTM_LSTM(number_tool.get_dim(), 5, number_tool.get_dim(),\n",
    "               memory_banks, memory_dim, output_length = 3)\n",
    "ntm.to(device)\n",
    "\n",
    "ntm_optim = torch.optim.SGD(ntm.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "ntm_kwargs = setup_kwargs.copy()\n",
    "ntm_kwargs['model'] = ntm\n",
    "ntm_kwargs['optim'] = ntm_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(**ntm_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Functional Testing\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing _convolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for _convolve\n",
    "s = torch.tensor([1.,0.,0.])\n",
    "x = torch.tensor([0.40, .05, .50, 0, 0, .05])\n",
    "\n",
    "print(x)\n",
    "_convolve(x,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Sequence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for criterion = nn.NLLLoss()\n",
    "\n",
    "n = Numbers(increment_func = lambda incr: randint(0,9), goal_func = lambda stream: stream[0:3])\n",
    "stream = n.create_stream(30)\n",
    "enc_stream = n.encode_stream(stream)\n",
    "\n",
    "goal = n.get_stream_goal(stream)\n",
    "target = torch.LongTensor(goal)\n",
    "\n",
    "l = BasicRNN(n.get_dim(), 5, n.get_dim(), 3)\n",
    "hid = l.init_hidden()\n",
    "\n",
    "the_out, the_hid = l.forward(enc_stream, hid)\n",
    "\n",
    "the_out[0,0,0] = -2#5\n",
    "the_out[1,0,5] = -2#5\n",
    "the_out[2,0,6] = -2#5\n",
    "\n",
    "loss = criterion(the_out.squeeze(1), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ_MEMORY TESTS\n",
    "mem = Memory(10,4)\n",
    "\n",
    "flat = torch.ones(10)\n",
    "just_4 = torch.zeros(10)\n",
    "just_4[4] = 1\n",
    "\n",
    "act = mem.read(just_4)\n",
    "exp = mem.memory[4]\n",
    "print(f'[RM TEST1] act:{act} exp:{exp}')\n",
    "\n",
    "act = mem.read(flat)\n",
    "exp = mem.memory.mean(dim=0)\n",
    "print(f'[RM TEST2] act:{act} exp:{exp}')\n",
    "\n",
    "# WRITE_MEMORY TESTS\n",
    "mem_dim = 3\n",
    "\n",
    "att_flat = torch.ones(10)\n",
    "att_just_4 = torch.zeros(10)\n",
    "att_just_4[4] = 1\n",
    "\n",
    "forget_ones = torch.ones(mem_dim)\n",
    "forget_half = torch.ones(mem_dim) / 2\n",
    "forget_zeros = torch.zeros(mem_dim)\n",
    "\n",
    "add_zero = torch.zeros(mem_dim)\n",
    "add_ones = torch.ones(mem_dim)\n",
    "\n",
    "mem = Memory(10, mem_dim)\n",
    "exp = mem.memory[4]\n",
    "act = mem.write(att_just_4, forget_zeros, add_zero)[4]\n",
    "print(f'[WM TEST1] act:{act} exp:{exp}')\n",
    "\n",
    "mem = Memory(10, mem_dim)\n",
    "exp = mem.memory[4] + 1\n",
    "act = mem.write(att_just_4, forget_zeros, add_ones)[4]\n",
    "print(f'[WM TEST2] act:{act} exp:{exp}')\n",
    "\n",
    "mem = Memory(10, mem_dim)\n",
    "exp = torch.zeros_like(mem.memory[4])\n",
    "act = mem.write(att_just_4, forget_ones, add_zero)[4]\n",
    "print(f'[WM TEST3] act:{act} exp:{exp}')\n",
    "\n",
    "mem = Memory(10, mem_dim)\n",
    "exp = mem.memory * (1 - 0.5 / 10)\n",
    "act = mem.write(att_flat, forget_half, add_zero)\n",
    "print(f'[WM TEST4] act:{act.view(-1)} exp:{exp.view(-1)}')\n",
    "\n",
    "mem = Memory(10, mem_dim)\n",
    "exp = mem.memory * (1 - 0.5 / 10) + 0.1\n",
    "act = mem.write(att_flat, forget_half, add_ones)\n",
    "print(f'[WM TEST5] act:{act.view(-1)} exp:{exp.view(-1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer Registration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Test, self).__init__()\n",
    "        self.y1 = torch.ones(5,5)\n",
    "        self.register_buffer('y2', torch.ones(5,5))\n",
    "        \n",
    "class Base(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Base, self).__init__()\n",
    "        self.x1 = torch.ones(5,5)\n",
    "        self.register_buffer('x2', torch.ones(5,5))\n",
    "        self.add_module('test', Test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Base()\n",
    "bc = b.to(device)\n",
    "\n",
    "print(bc.x1.device)\n",
    "print(bc.x2.device)\n",
    "print(bc.test.y1.device)\n",
    "print(bc.test.y2.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
